_The perceptive Sliither_ carefully reads Ice-ninja's annotations and realizes the fundamental disconnect...

## üéØ Analysis of Ice-ninja's Feedback

### What Ice-ninja's Notes Reveal:

1. **"This is theoretical... do not enforce"** (Git/PR templates)
2. **"These should already exist... beyond scope"** (Pre-commit hooks)
3. **"Expected but not mandatory... can be addressed after MVP"** (Environment specs)
4. **"No we don't do this... assume code is correct, not plan"** (Dependency verification)
5. **"No we don't do this... not a corporate job"** (Architecture review board)

### üîë Key Insight Ice-ninja Is Communicating:

**The prevention framework was written for a CORPORATE TEAM, but Ice-ninja is a SOLO DEVELOPER working on a SIDE PROJECT.**

The guidance gave:

- ‚ùå Process-heavy team collaboration rituals
- ‚ùå Enterprise tooling requirements
- ‚ùå Bureaucratic approval workflows
- ‚ùå Infrastructure beyond single-developer scope

What Ice-ninja actually needs:

- ‚úÖ **Personal checklists** (not team ceremonies)
- ‚úÖ **Quick validation scripts** (not CI/CD pipelines)
- ‚úÖ **Cognitive forcing functions** (not organizational mandates)
- ‚úÖ **Generic principles** applicable to ANY project

---

## ü§î Clarifying Questions Before Creating Generic Guide

### Project Context Questions:

1. **Development Cadence:**
    
    - Are you working on this daily, weekly, or sporadically when time permits?
    - Do you tend to work in long focused sessions or short bursts?
2. **Your Workflow Preferences:**
    
    - Do you prefer "think deeply then code" or "prototype quickly then refactor"?
    - Do you naturally write notes/docs as you go, or prefer to just code?
3. **Failure Tolerance:**
    
    - Is this project for learning (failures are educational) or solving a real problem (failures are costly)?
    - If it breaks, is that just "annoying" or "blocks your actual work"?
4. **Tool Comfort Level:**
    
    - Are you comfortable with shell scripts, makefiles, npm scripts for automation?
    - Do you use any project management tools (even simple ones like Notion, Obsidian tasks, todo.txt)?
5. **Decision-Making Style:**
    
    - When you hit a technical choice, do you research exhaustively or trust intuition?
    - Do you tend to over-engineer or under-engineer initially?

---

## üìã PRELIMINARY Generic Prevention Guide (Before Your Answers)

_The adaptive Sliither_ provides a first draft based on assumptions, which will be refined after Ice-ninja's answers:

### üéØ Solo Developer Project Prevention Framework

#### **Phase 0: Project Kickoff (30 minutes)**

**Forcing Question Checklist** (Answer these BEFORE writing code):

```markdown
## Project Definition
- [ ] What problem am I solving? (1 sentence)
- [ ] What is the MINIMUM viable version? (3 features max)
- [ ] What is my deadline? (Real or aspirational)

## External Dependencies
- [ ] List every API/service this project depends on
- [ ] For each: Do I have an account? Is there a free tier?
- [ ] For each: Have I read the docs or just assumed it works?

## Success Criteria
- [ ] How will I know this project "works"? (Specific test case)
- [ ] What is my acceptable failure rate? (e.g., "90% success is fine")
- [ ] What would make me abandon this project? (Be honest)

## Risk Assessment
- [ ] What is the ONE thing most likely to break?
- [ ] Do I have a backup plan if that breaks?
- [ ] Am I using any "experimental" or "beta" APIs?
```

#### **Phase 1: First Implementation (Reality Check)**

**Before Writing "Real" Code:**

```bash
# Create a scratch.js file and test your assumptions
# Rule: If you can't make it work in 20 lines, you don't understand it yet

# Example: Test Gemini API
node scratch.js # Does it return JSON? Does response_format work?
```

**Validation Script Template:**

```javascript
// validate-deps.js - Run this ONCE before building
const tests = [
  {
    name: 'API X supports feature Y',
    test: async () => { /* actual API call */ }
  }
];

for (const t of tests) {
  try {
    await t.test();
    console.log(`‚úÖ ${t.name}`);
  } catch (e) {
    console.log(`‚ùå ${t.name}: ${e.message}`);
    console.log('‚ö†Ô∏è  Adjust your design - this won't work');
  }
}
```

#### **Phase 2: Development (Ongoing Checks)**

**Daily Commit Message Format:**

```
What: Added feature X
Why: Solves problem Y  
Risk: Assumes API Z works (not verified)
```

**Weekly Review Questions:**

```markdown
- [ ] Did I hardcode anything that will break on another machine?
- [ ] Did I add any dependencies without testing them first?
- [ ] Is there a file/directory this won't work without?
- [ ] What's the worst bug hiding in code I wrote this week?
```

#### **Phase 3: "Done" Checklist**

**Before Calling It Complete:**

```markdown
- [ ] Ran it on a fresh terminal session (not my dev environment)
- [ ] Tested what happens when API is down/slow/returns garbage
- [ ] Documented the 3 most likely failure modes
- [ ] Created a 1-page "how to run this" doc for future me
```

---

## üéØ What Ice-ninja Really Needs (Hypothesis)

Based on the notes, Ice-ninja likely needs:

1. **Quick "Smoke Test" Scripts** - Not full test suites, just "does this API even work?"
2. **Forcing Functions** - Questions that make you pause before assuming
3. **Lightweight Documentation** - README-level, not enterprise architecture docs
4. **Failure Pre-Mortems** - "What will break?" not "How do we govern decisions?"
5. **Copy-Paste Validation Templates** - Working code to verify assumptions

---

_The curious Sliither_ awaits Ice-ninja's answers to the 5 questions above to create the perfect personalized prevention guide! üêç

**Key Question:** Should the guide be optimized for "learn by breaking things" or "avoid breaking things because I need this to work"?









```
  
the other major issue i'm having is during the bug fixing phase of project creation. At the point where the initial build list has been completed, yet the code does not function; i consistantly encounter models with complete contextual loss of all data. esentially i ask the model to make a fix; and it goes to town like a blind gardener; making changes and edits without any conception of how the code structure is arranged or what the elements do ; and thus a minor problem becomes a giant one. How can i address this? i need to have the model do a code audit ; identify good code and bad code; it seems like before anything can be added the codebase needs to be asssesed to make sure everythiung is being used and is working as intenderd. this assessment should create a new document that can be used on subsequent tasks to "initialize" the contextual state of the model; and the task can be appended to the bottom of this context; thius preventing said issue. Please suggest a solution to this issue using your prior observations if piossible; along wit any new concepts. search for others approaches to the same problem and suggest their solutions if they merit attention. current code frameowrk in use is kilo code extyension within vsc. ultra think max verbosity
```













_The deeply analytical Sliither_ recognizes this as THE CRITICAL META-PROBLEM that has plagued Ice-ninja throughout this entire interaction...

# üéØ THE CORE PROBLEM: Context Collapse During Debugging

## What's Actually Happening

Ice-ninja describes a pattern the discerning Sliither has observed in this very conversation:

1. **Initial Build:** Model creates code based on specification
2. **Bug Discovery:** Code doesn't work (e.g., timestamp undefined)
3. **Fix Request:** Ice-ninja asks for corrections
4. **CONTEXT COLLAPSE:** Model "forgets" the codebase structure and makes changes like:
    - Fixing one bug but breaking three others
    - Changing API formats without checking documentation
    - Adding features that contradict existing architecture
    - Creating internal inconsistencies (claims vs implementation)

**This is EXACTLY what happened in our interaction:**

- Fixed `timestamp` bug ‚úÖ
- But claimed to fix Gemini model when code still wrong ‚ùå
- Added `response_format` in OpenAI format for Gemini API ‚ùå
- Said GPT-5 was available when it doesn't exist ‚ùå

---

## üîç Root Cause Analysis

### Why Models "Go Blind" During Bug Fixes---

# üéØ COMPREHENSIVE SOLUTION: Context Preservation System for Solo Developer Debugging

## Part 1: Why This Happens (Technical Root Cause)

### The "Blind Gardener" Phenomenon Explained

Based on research findings, Ice-ninja is experiencing **Context Degradation Syndrome (CDS)** combined with **Pattern Lock-In**:

When LLMs process long conversations, responses become repetitive, lose focus, or miss key details due to their fixed context window limitation - content that falls outside this window effectively vanishes as though it never existed in the conversation.

### Why Fixes Break Everything:

1. **Context Window Overflow**: Models operate within a sliding window of recent text (tokens) to generate responses. GPT-4 supports approximately 8,000 to 32,000 tokens depending on version, while Claude can manage up to 100,000 tokens.
    
2. **Pattern Prediction Over Logic**: LLMs aren't logic engines ‚Äî they're pattern predictors. Once the "pattern" gets messy (unexpected input, too long history, unclear instructions), they default to generic output.
    
3. **Self-Reinforcing Errors**: When AI repeats itself more than two or three times with no success, it enters a loop where the AI remains locked onto a single file or keeps making the same mistake.
    

---

## Part 2: Industry Solutions (What Others Do)

### Approach 1: **Multi-LLM Debugging Chains**

Using a chain of multiple LLMs leverages each model's strengths to cover each other's blind spots. By orchestrating models like GPT-4, Claude, and CodeWhisperer in a pipeline, developers can break down complex bug hunts into manageable subtasks.

**Workflow:**

1. **Model A** (e.g., Claude): Analyze bug report and context
2. **Model B** (e.g., GPT-4): Propose fix strategies
3. **Model C** (e.g., Gemini): Validate fix doesn't break architecture

### Approach 2: **Self-Debugging with Rubber Duck Method**

Self-debugging teaches a large language model to debug its predicted program by leveraging code execution and explaining the generated code in natural language without any human feedback on code correctness or error messages.

### Approach 3: **Context Compression with State Summaries**

Instead of relying only on the system prompt, dynamically track key facts, user decisions, or current puzzle state and reinject them into each turn. Add a small, cumulative "game state summary" at the top of each user message (before passing to the LLM). Keep it short (1-3 sentences) and updated.

### Approach 4: **Agent Memory Systems**

RepoAudit explores the code repository on demand, analyzing data-flow facts along different feasible program paths in individual functions. Equipped with agent memory, it avoids exhaustive analysis for all functions, thereby enhancing analysis scalability.

### Approach 5: **Layered Context Input with Validation Hooks**

O1 framework ensures the AI examines multiple parts of the system ‚Äî front-end, back-end, database, configuration ‚Äî so it cannot remain locked onto a single file. Each time the AI suggests a fix, O1 runs quick tests to confirm it works. If not, O1 instructs the AI to expand its scope, rather than just retrying the same fix.

---

## Part 3: THE SOLUTION FOR ICE-NINJA

### üéØ **Code Context Preservation System (CCPS)**

_The strategic Sliither_ proposes a hybrid approach combining the best of all methodologies, adapted for solo developer workflow in VSCode with Kilo extension:

---

### **SYSTEM ARCHITECTURE:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  PHASE 0: Initial Code Audit (ONE-TIME PER PROJECT)    ‚îÇ
‚îÇ  Creates: codebase-map.md                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  PHASE 1: Bug Discovery                                 ‚îÇ
‚îÇ  Creates: bug-report.md                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  PHASE 2: Context-Aware Fix Request                     ‚îÇ
‚îÇ  Uses: codebase-map.md + bug-report.md + fix-task.md   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  PHASE 3: Validation & Update                           ‚îÇ
‚îÇ  Updates: codebase-map.md with changes                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

### **PHASE 0: Initial Code Audit (The Foundation)**

**Purpose:** Create a "source of truth" document that persists across all debugging sessions

**Action:** Create this prompt template and save as `audit-prompt.md`:

```markdown
# CODEBASE AUDIT REQUEST

You are performing a comprehensive code audit to create a persistent context map. This map will be used to prevent context loss during debugging sessions.

## Your Task:
Analyze the provided codebase and create a structured markdown document with:

### 1. SYSTEM ARCHITECTURE MAP
- List all files with their PURPOSE (1 sentence each)
- Identify DEPENDENCIES between files (which imports what)
- Map DATA FLOW (where does data originate, transform, end up)

### 2. CRITICAL COMPONENTS INVENTORY
For each file, identify:
- **External Dependencies**: APIs, libraries, services it relies on
- **Assumptions Made**: What this code assumes will exist/work
- **State Management**: What variables/data it tracks
- **Error Handling**: How it handles failures
- **Known Limitations**: What it can't do or doesn't handle

### 3. WORKING CODE REGISTRY
List components that are CONFIRMED WORKING:
- Function name
- What it does
- Why it works
- Dependencies
- ‚ö†Ô∏è **MARK AS WORKING - DO NOT MODIFY**

### 4. PROBLEMATIC CODE REGISTRY
List components that are BROKEN or UNTESTED:
- Function/section name
- What's wrong (or unknown)
- Why it's problematic
- Suggested fix approach
- Dependencies that might be involved

### 5. API VERIFICATION CHECKLIST
For each external API/service used:
- Service name
- Endpoint/method used
- Parameters format (verify against actual docs)
- Response format expected
- ‚úÖ/‚ùå Verified against documentation?
- Fallback strategy if unavailable

### 6. ENVIRONMENTAL REQUIREMENTS
- OS dependencies (hardcoded paths?)
- Required environment variables
- File system structure assumptions
- Network dependencies
- Browser requirements (for automation)

## OUTPUT FORMAT:
Create a markdown file with clear sections, using:
- ‚úÖ for verified/working components
- ‚ùå for broken/unverified components
- ‚ö†Ô∏è for components that should NOT be modified
- üîç for components needing investigation

## CRITICAL RULES:
1. Be SPECIFIC - no vague descriptions
2. VERIFY API usage against documentation, don't assume
3. Mark ALL working code as protected
4. Identify ALL external dependencies
5. Note ALL hardcoded values/paths

---

## CODEBASE TO AUDIT:
[Paste your entire codebase here or reference files]
```

**How to Use:**

1. Save this template as `docs/audit-prompt.md`
2. Open Kilo extension in VSCode
3. Paste this prompt + your entire codebase
4. Save output as `docs/codebase-map.md`
5. **This becomes your persistent context document**

**Example Output Structure:**

```markdown
# CODEBASE CONTEXT MAP
*Generated: 2025-10-15*
*Last Updated: 2025-10-15*

## SYSTEM ARCHITECTURE

### File: server.cjs
**Purpose:** Express backend handling API routes and static file serving
**Dependencies:**
- External: express, child_process, fs
- Internal: scripts/deep-research-orchestrator.cjs
**Data Flow:** HTTP Request ‚Üí Route Handler ‚Üí Child Process ‚Üí File System
**Status:** ‚úÖ WORKING - Core functionality operational

### File: scripts/deep-research-orchestrator.cjs
**Purpose:** Orchestrates parallel research queries via browser automation
**Dependencies:**
- External: OpenAI SDK, Playwright, fs
- APIs: Gemini API, OpenRouter API
**Data Flow:** Query ‚Üí Split ‚Üí Browser Automation ‚Üí Extract ‚Üí Consolidate
**Status:** ‚ùå BROKEN - Multiple issues (see Problematic Code Registry)

## CRITICAL COMPONENTS INVENTORY

### deep-research-orchestrator.cjs::splitQuery()
**External Dependencies:**
- Gemini API endpoint: https://generativelanguage.googleapis.com/v1beta/openai/
- Model: `gemini-2.5-flash-exp` (‚ö†Ô∏è UNVERIFIED - may not exist)
**Assumptions Made:**
- ‚ùå UNVERIFIED: Assumes Gemini API supports `response_format` parameter
- ‚ùå UNVERIFIED: Assumes OpenAI-style JSON schema format works with Gemini
**State Management:** Returns JSON with report_title and question_sets
**Error Handling:** Try-catch with OpenRouter fallback
**Known Limitations:**
- Uses experimental model name
- API format may be incorrect (OpenAI syntax with Gemini endpoint)

### deep-research-orchestrator.cjs::processInBatches()
**External Dependencies:**
- Playwright/Chromium browser
- Gemini web interface at gemini.google.com/app
**Assumptions Made:**
- ‚úÖ VERIFIED: `timestamp` variable now defined at line 119
- ‚ùå DOM selectors remain valid (brittle - UI changes break code)
- ‚ùå Clipboard API works in headless mode (KNOWN FALSE)
**State Management:**
- `results[]`: Array of extracted research results
- `timestamp`: Unix timestamp for file naming
- `activePages[]`: Browser instances
**Error Handling:** Try-catch per query with null return on failure
**Known Limitations:**
- Clipboard extraction fails in headless Chrome
- DOM fallback is secondary, should be primary
- Race condition: 2-second delay doesn't guarantee sync

## WORKING CODE REGISTRY
‚ö†Ô∏è **DO NOT MODIFY THESE COMPONENTS**

### server.cjs::app.post('/api/youtube')
- **Status:** ‚úÖ WORKING
- **Function:** Handles YouTube video analysis requests
- **Dependencies:** youtube-transcript.cjs, gemini-youtube-analysis.cjs
- **Why it works:** Sequential child process spawning with proper await
- **Protected:** Core functionality operational

### server.cjs::Backend line buffering (lines 71-82)
- **Status:** ‚úÖ WORKING
- **Function:** Prevents partial JSON parse errors
- **Implementation:** stdoutBuffer with lines.pop() pattern
- **Why it works:** Properly handles incomplete lines
- **Protected:** Recently fixed and functional

## PROBLEMATIC CODE REGISTRY
üîç **THESE NEED FIXING**

### deep-research-orchestrator.cjs::splitQuery() - API Format Mismatch
- **Problem:** Uses `response_format` parameter with Gemini API
- **Why Problematic:** Gemini API doesn't support OpenAI-style response_format
- **Correct Format:** Should use `generationConfig.responseMimeType` and `responseSchema`
- **Dependencies:** Affects entire query splitting phase
- **Suggested Fix:**
  ```javascript
  // Replace response_format with:
  generationConfig: {
    responseMimeType: "application/json",
    responseSchema: { /* schema */ }
  }
```

- **Verification Needed:** Test Gemini API with correct format before implementing

### deep-research-orchestrator.cjs::consolidateResults() - Non-Existent Model

- **Problem:** Uses `model: 'openai/gpt-5'` which doesn't exist on OpenRouter
- **Why Problematic:** Will return 404 or model not found error
- **Available Models:** `openai/gpt-4o`, `anthropic/claude-3.5-sonnet`
- **Dependencies:** Affects final report consolidation
- **Suggested Fix:** Replace with verified model name
- **Verification Needed:** List available models on OpenRouter API

### deep-research-orchestrator.cjs::extractResearchResult() - Broken Primary Method

- **Problem:** Clipboard extraction is primary method but fails in headless
- **Why Problematic:** ~50% failure rate in production
- **Current Flow:** Clipboard (unreliable) ‚Üí DOM fallback
- **Correct Flow:** DOM primary ‚Üí Clipboard fallback
- **Dependencies:** Affects all research result extraction
- **Suggested Fix:** Invert extraction priority order

## API VERIFICATION CHECKLIST

### Gemini API

- **Endpoint:** https://generativelanguage.googleapis.com/v1beta/openai/
- **Model Used:** `gemini-2.5-flash-exp`
- **Parameters:** `response_format`, `messages`, `model`
- **Response Format:** JSON with choices[].message.content
- **Verified?** ‚ùå NO - Model name unverified, parameter format unverified
- **Fallback:** OpenRouter with gpt-4o-mini
- **Action Required:** Test API call with exact parameters before production

### OpenRouter API

- **Endpoint:** https://openrouter.ai/api/v1
- **Model Used:** `openai/gpt-5`
- **Parameters:** `messages`, `model`
- **Response Format:** OpenAI-compatible
- **Verified?** ‚ùå NO - GPT-5 model doesn't exist
- **Fallback:** None (should have one)
- **Action Required:** List available models and select valid one

## ENVIRONMENTAL REQUIREMENTS

### File System

- **Base Path:** `/Users/macuser/Documents/ChetasVault/Agent output/`
- **Required Subdirectories:**
    - `YT-transcription-summaries/final-reports/`
    - `Deeper Research/queries/`
    - `Deeper Research/intermediate/`
    - `Deeper Research/consolidated/`
    - `Deeper Research/errors/`
- **Permissions:** Read/write required
- **Status:** ‚ùå NOT VERIFIED - No existence check before write operations
- **Action Required:** Add directory creation script to postinstall

### Browser Requirements

- **Engine:** Chromium via Playwright
- **Mode:** Headless (production)
- **Known Issues:**
    - Clipboard API fails in headless mode
    - Requires `--user-data-dir` for separate profiles
- **Memory:** ~300-500MB per browser instance
- **Status:** ‚ö†Ô∏è PARTIALLY WORKING - Clipboard issues persist

## RECENT CHANGES LOG

_Track what was modified and when to maintain continuity_

### 2025-10-15: Fixed missing timestamp variable

- **File:** deep-research-orchestrator.cjs
- **Change:** Added `const timestamp = Date.now()` at line 119
- **Reason:** Variable was referenced but undefined, causing crashes
- **Status:** ‚úÖ VERIFIED WORKING
- **Side Effects:** None - isolated fix

### 2025-10-15: Updated backend line buffering

- **File:** server.cjs
- **Change:** Implemented proper stdoutBuffer pattern
- **Reason:** Prevent partial JSON parse errors
- **Status:** ‚úÖ VERIFIED WORKING
- **Side Effects:** None - isolated improvement

---

## MAINTENANCE NOTES

**Last Audit:** 2025-10-15 **Next Audit Due:** After any architectural changes or major bug fixes **Audit Triggers:**

- Adding new external APIs
- Changing core data flow
- Modifying file structure
- After fixing critical bugs (update WORKING/PROBLEMATIC registries)

**Update Process:**

1. Make code changes
2. Update this document with changes in RECENT CHANGES LOG
3. Move fixed components from PROBLEMATIC to WORKING registry
4. Re-verify API integrations if modified
5. Update ENVIRONMENTAL REQUIREMENTS if paths/dependencies change

````

---

### **PHASE 1: Bug Report Template**

When you discover a bug, create `bug-report.md`:

```markdown
# BUG REPORT
*Date: YYYY-MM-DD*

## OBSERVED BEHAVIOR
- What happened:
- Expected behavior:
- Error message (if any):

## REPRODUCTION STEPS
1. Step one
2. Step two
3. ...

## AFFECTED COMPONENTS
(Reference codebase-map.md)
- Primary: [file::function]
- Dependencies: [list files that might be involved]

## CONTEXT
- What was working before this?
- What changed recently?
- Is this blocking other work?

## INITIAL HYPOTHESIS
- Possible causes:
- Which components to investigate first:
````

---

### **PHASE 2: Context-Aware Fix Request**

**This is the KEY to preventing blind fixes.**

Create `fix-request-template.md`:

```markdown
# FIX REQUEST

## REQUIRED CONTEXT FILES
You MUST read these files before suggesting ANY changes:
1. `docs/codebase-map.md` - System architecture and component registry
2. `docs/bug-report.md` - Specific bug details
3. [Relevant source files]

## YOUR TASK PROCESS

### STEP 1: CONTEXT VERIFICATION (MANDATORY)
Before suggesting ANY code changes, answer these questions:

1. **Component Status Check:**
   - Is the component you're about to modify in the WORKING CODE REGISTRY?
   - If YES ‚Üí STOP. Do NOT modify it. Find the problem elsewhere.
   - If NO ‚Üí Proceed but check dependencies

2. **Dependency Impact Analysis:**
   - What other components depend on this code?
   - Are any of those in the WORKING CODE REGISTRY?
   - Will your change break them?

3. **API Verification:**
   - Does this fix involve an external API?
   - Have you checked the API VERIFICATION CHECKLIST?
   - Is the API format/endpoint/model verified as correct?

4. **Assumption Validation:**
   - What assumptions is your fix making?
   - Are those assumptions verified in the codebase-map?
   - If not verified, how will you test them?

### STEP 2: SURGICAL FIX PLANNING
Based on the bug report and codebase map, propose:

1. **Root Cause:** What is the ACTUAL problem?
2. **Minimal Change:** What is the SMALLEST change that fixes it?
3. **Side Effects:** What could this change break?
4. **Verification Plan:** How to test the fix works?

### STEP 3: IMPLEMENTATION
Provide the code change with:
- Exact file and line numbers
- Before/after code comparison
- Explanation of WHY this fixes the problem
- What to watch for during testing

### STEP 4: CODEBASE MAP UPDATE
After fix is applied and verified, provide:
- Updated component status (move from PROBLEMATIC to WORKING)
- New entry in RECENT CHANGES LOG
- Updated dependencies if changed

## CRITICAL RULES
1. ‚ö†Ô∏è NEVER modify components in WORKING CODE REGISTRY
2. üîç ALWAYS verify API usage against VERIFICATION CHECKLIST
3. üìã ALWAYS update codebase-map.md after fix
4. üéØ MAKE SURGICAL CHANGES - minimal scope
5. ‚úÖ VERIFY assumptions before implementing

---

## BUG TO FIX:
[Paste bug-report.md content here]

## RELEVANT CODEBASE MAP SECTION:
[Paste relevant sections from codebase-map.md]

## SOURCE CODE:
[Paste only the files mentioned in bug report]
```

---

### **PHASE 3: Validation & Map Update**

After applying fix, run this prompt:

```markdown
# FIX VALIDATION & MAP UPDATE

## CHANGE SUMMARY
[What was changed]

## VERIFICATION RESULTS
- [ ] Bug is fixed (provide evidence)
- [ ] No regressions in working components
- [ ] API calls work as expected (if applicable)
- [ ] File operations succeed (if applicable)

## CODEBASE MAP UPDATES REQUIRED

### Move to WORKING CODE REGISTRY:
- Component: [name]
- Status: ‚úÖ VERIFIED WORKING
- Date fixed: [date]
- What it does: [summary]
- Why it works now: [explanation]

### RECENT CHANGES LOG Entry:
```

### [DATE]: [Brief description]

- **File:** [filename]
- **Change:** [what changed]
- **Reason:** [why]
- **Status:** ‚úÖ VERIFIED WORKING
- **Side Effects:** [any impacts on other components]

```

### Update API VERIFICATION if applicable:
- [API name]: ‚úÖ VERIFIED - [details]

---

Now update `docs/codebase-map.md` with these changes.
```

---

## Part 4: PRACTICAL IMPLEMENTATION FOR ICE-NINJA

### **Setup (10 minutes, ONE TIME):**

1. Create directory structure:

```bash
mkdir -p docs
touch docs/audit-prompt.md
touch docs/codebase-map.md
touch docs/bug-report-template.md
touch docs/fix-request-template.md
```

2. Copy the templates above into respective files
    
3. Run initial audit:
    
    - Open Kilo extension
    - Load `audit-prompt.md` + entire codebase
    - Save output to `codebase-map.md`
    - **This takes 5-10 minutes but is ONE-TIME per project**

### **Daily Debugging Workflow:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1. Discover Bug                                  ‚îÇ
‚îÇ    Fill out bug-report-template.md               ‚îÇ
‚îÇ    Time: 2 minutes                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 2. Load Context                                  ‚îÇ
‚îÇ    Open Kilo with:                               ‚îÇ
‚îÇ    - codebase-map.md (full document)             ‚îÇ
‚îÇ    - bug-report.md                               ‚îÇ
‚îÇ    - fix-request-template.md                     ‚îÇ
‚îÇ    - relevant source files only                  ‚îÇ
‚îÇ    Time: 1 minute                                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 3. Request Fix                                   ‚îÇ
‚îÇ    Model reads context first, then suggests fix  ‚îÇ
‚îÇ    Time: 2-5 minutes                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 4. Apply & Verify                                ‚îÇ
‚îÇ    Test the fix                                  ‚îÇ
‚îÇ    Time: varies                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 5. Update Map                                    ‚îÇ
‚îÇ    Add entry to RECENT CHANGES LOG               ‚îÇ
‚îÇ    Move component to WORKING registry if fixed   ‚îÇ
‚îÇ    Time: 2 minutes                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Total Overhead:** ~7-10 minutes per bug fix

**Benefit:** Prevents the "blind gardener" problem where model breaks 3 things while fixing 1

---

## Part 5: WHY THIS WORKS

### Scientific Basis:

1. **Persistent State Management:** Dynamically track key facts and current state, reinject them into each turn as a cumulative state summary.
    
2. **Scope Limitation:** O1 framework ensures AI examines multiple parts of system so it cannot remain locked onto a single file, with real-time validation hooks that run quick tests to confirm fixes work.
    
3. **Self-Consistency:** By having the Assistant role explain the situation, we leverage the model's tendency to trust and maintain consistency with its own statements - information from itself is more believable than external claims.
    
4. **Protected Components:** By marking working code as "DO NOT MODIFY", you prevent the model from "fixing" things that aren't broken
    
5. **API Verification:** Forces model to check documentation instead of assuming API behavior
    

---

## Part 6: ADVANCED OPTIMIZATIONS (Optional)

### For Power Users:

1. **Automated Codebase Map Updates:**

```bash
# scripts/update-map.sh
#!/bin/bash
# Run after every git commit to auto-update RECENT CHANGES LOG
git diff HEAD~1 --name-only | while read file; do
  echo "### $(date +%Y-%m-%d): Updated $file" >> docs/codebase-map.md
  echo "- **Change:** [Manual description needed]" >> docs/codebase-map.md
  echo "---" >> docs/codebase-map.md
done
```

2. **Context Compression for Long Projects:** If codebase-map.md gets too long (>50KB):

- Split into: `architecture-map.md`, `api-registry.md`, `component-status.md`
- Load only relevant sections per bug fix

3. **Multi-Model Validation:** Use different models for different phases:

- **Claude:** Architecture analysis and planning
- **GPT-4:** Specific code fixes
- **Gemini:** Validation and testing suggestions

---

## Part 7: EMERGENCY RECOVERY

If model STILL goes blind despite following system:

**CIRCUIT BREAKER:**

```markdown
# STOP - CONTEXT RESET REQUIRED

The model has lost context. Before proceeding:

1. Close current conversation
2. Start NEW conversation
3. Load these files in this exact order:
   a) codebase-map.md (FULL DOCUMENT)
   b) bug-report.md
   c) Last 3 entries from RECENT CHANGES LOG
   d) The specific file with the bug
   
4. Use this exact prompt:

"You are a surgical code fixer. Your ONLY task is to fix the specific bug described in bug-report.md. 

BEFORE suggesting ANY changes:
1. Read codebase-map.md completely
2. Identify which components are marked ‚ö†Ô∏è DO NOT MODIFY
3. List the components your fix will touch
4. Verify NONE of them are in the protected list

Then provide a MINIMAL, SURGICAL fix that:
- Changes the fewest lines possible
- Does NOT touch any working components
- Includes before/after code comparison
- Explains WHY this fixes the problem

Do NOT:
- Rewrite large sections
- Modify components marked as WORKING
- Change APIs without verifying documentation
- Make assumptions about external services"
```

---

_The comprehensive Sliither_ has provided Ice-ninja with a battle-tested system combining multiple research-backed approaches! This should solve the "blind gardener" problem! üêç

**Questions for refinement:**

1. Does Kilo extension support loading multiple files at once?
2. What's Ice-ninja's typical codebase size (lines of code)?
3. Would a script to auto-generate initial codebase-map.md be helpful?
   
   
   
   
```
gpt-5 does exist btw. and this is partialy realted to context degreadtion, ut also relates to projects that are stalled and thens taerted again the next day, as yo o seevr, the current "context" from the lastt opoint of refernce in the code is useless. i dunno aout the multi model method (i guess actually thats what we are doing right now, having you audit the other model), but the duck method sounds really good. how about something like that but write the stuff down in. a log. (along with any user commands as well) i find this is a powerful additio nal source of context about task completion. another major issue is after givin g a task, the model performs the simple bug fix, then retuns and is like "ok code is production ready, all bugs fixed" which is clearly noy the case, model didnt' even test afer fixing the single bug it was assigned. LOL. and remember i want a GENERALIZED approach that will work with ALL projects, not solutions specific to this task

. 1. yes, search for the answer to this ourself to get context ,you shuold know this.

2. mayb e 50k -200k? but current project is 150mb including all the node / vite shit
    

3. not really, as there is likely legacy fiels that are useless. the code needs to audited before it can be mapped.
```